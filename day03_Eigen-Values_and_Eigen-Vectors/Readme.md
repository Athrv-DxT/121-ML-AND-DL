# Day 3: Eigenvalues, Eigenvectors & PCA Basics  

## ðŸ“Œ Agenda  
- Eigenvalues & Eigenvectors  
- Spectral Theory Basics  
- Principal Component Analysis (PCA) Fundamentals  
- Applications in Machine Learning  

## ðŸ§¾ Key Concepts  

### ðŸ”¹ Eigenvalues & Eigenvectors  
- Describe how a transformation changes data along specific directions.  
- **Importance in ML:** Help in data transformation, stability analysis, and dimensionality reduction.  

---

### ðŸ”¹ Spectral Theory Basics  
- Decomposes matrices into simpler components using eigenvalues and eigenvectors.  
- **Importance in ML:** Forms the backbone of PCA, SVD, and other data analysis techniques.  

---

### ðŸ”¹ PCA (Principal Component Analysis) Basics  
- Reduces the dimensionality of data while keeping maximum variance.  
- **Importance in ML:** Improves training speed, reduces noise, and helps with visualization.  

---

### ðŸ”¹ Applications in ML  
- Data compression  
- Noise reduction  
- Visualization of high-dimensional data  
- Preprocessing before model training  

