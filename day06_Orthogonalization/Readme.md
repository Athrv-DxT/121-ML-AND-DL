# Day 6: Orthogonalization & Gram-Schmidt Process  

## ðŸ“Œ Agenda  
- Orthogonalization Concept  
- Gram-Schmidt Algorithm  
- Orthonormal Basis Coding & Visualization  
- Practical Applications in Machine Learning  

---

## ðŸ§¾ Key Concepts  

### ðŸ”¹ Orthogonalization  
- The process of converting a set of vectors into mutually perpendicular (orthogonal) vectors.  
- Helps in simplifying computations and reducing redundancy in data representation.  

---

### ðŸ”¹ Gram-Schmidt Algorithm  
- Takes a set of linearly independent vectors and generates an orthonormal basis.  
- Formula:  
  u1 = v1  
  u2 = v2 âˆ’ proj(u1, v2)  
  u3 = v3 âˆ’ proj(u1, v3) âˆ’ proj(u2, v3) ...  
  where proj(ui, vj) = (vÂ·ui / uiÂ·ui) * ui  
- Final step: Normalize each vector to get orthonormal basis.  

---

### ðŸ”¹ Orthonormal Basis  
- A set of orthogonal vectors with unit length.  
- Ensures Qáµ€Q = I (Identity Matrix), which simplifies many matrix computations.  

---

### ðŸ”¹ Applications in ML  

1. **Feature Decorrelation:**  
   Gram-Schmidt can transform correlated features into uncorrelated ones, improving model performance in regression or PCA.  

2. **Dimensionality Reduction:**  
   Helps build orthonormal bases for techniques like PCA, making data representation more compact without losing key information.  

3. **QR Decomposition:**  
   Widely used in solving linear regression equations efficiently using matrix factorization (A = Q R).  

4. **Signal Processing & NLP:**  
   Used to generate orthonormal bases for noise reduction and extracting important features in text or audio data.  

---
