# Day 35 - Model Evaluation Metrics

In this session, I practiced three core evaluation metrics for classification models:

- **Accuracy**: Ratio of correctly predicted observations to total observations.  
- **Precision**: Ratio of correctly predicted positives to total predicted positives.  
- **Recall (Sensitivity)**: Ratio of correctly predicted positives to all actual positives.  

### Why they matter?
- Accuracy is good when classes are balanced.  
- Precision is important when false positives are costly (e.g., spam detection).  
- Recall is crucial when false negatives are costly (e.g., disease detection).  

### Advanced Add-on
Also explored:
- **F1 Score**: Harmonic mean of precision and recall.  
- **Classification Report**: Sklearnâ€™s utility for quick evaluation.  
- **Confusion Matrix heatmap** for visual analysis.  

---
