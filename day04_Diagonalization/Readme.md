# Day 4: Diagonalization & Eigen Decomposition  

## ðŸ“Œ Agenda  
- Diagonalization Concept & Conditions  
- Eigen Decomposition Coding  
- Visualization of Eigenvectors  
- Applications in Machine Learning  

## ðŸ§¾ Key Concepts  

### ðŸ”¹ Diagonalization  
- Process of writing a matrix \( A \) as \( A = P D P^{-1} \) where \( D \) is diagonal, and \( P \) contains eigenvectors.  
- **Importance in ML:** Simplifies complex matrix operations like powers, exponentials, and dimensionality reduction.  

---

### ðŸ”¹ Eigen Decomposition  
- Factorizes a matrix into eigenvalues (scaling factors) and eigenvectors (directions).  
- **Importance in ML:** Helps find principal directions in data, reduces complexity in large systems.  

---

### ðŸ”¹ Visualization of Eigenvectors  
- Eigenvectors show invariant directions under matrix transformations.  
- Plotting them helps us understand stretching, shrinking, or rotating effects on data.  

---

### ðŸ”¹ Applications in ML  
- **PCA (Principal Component Analysis):** Finds directions of maximum variance.  
- **Data Compression:** Reduces dimensionality while preserving important information.  
- **Noise Reduction:** Keeps dominant patterns, removes random variations.  
- **Model Optimization:** Used in optimization algorithms and spectral clustering.  
