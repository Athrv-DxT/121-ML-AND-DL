# Day 10: Quadratic Forms & Loss Surfaces Visualization  

## ğŸ“Œ Agenda  
- Quadratic Forms  
- Loss Surface Equations  
- Practical Usage in Machine Learning  

---

## ğŸ§¾ Key Concepts  

### ğŸ”¹ Quadratic Forms  
- Mathematical representation:  
  Q(x) = xáµ€ A x  
- General quadratic function:  
  f(x) = xáµ€ A x + báµ€ x + c  
- Where A is a symmetric matrix, x is a vector, and c is a constant term.  

---

### ğŸ”¹ Loss Function for Regression  
- Mean Squared Error (MSE) Loss:  
  L(w) = Â½ (Xw âˆ’ y)áµ€ (Xw âˆ’ y)  
- Where:  
  - X = Input features  
  - w = Weights  
  - y = Target values  

---

### ğŸ”¹ Convexity Condition  
- If A â‰» 0 (A is positive definite), then:  
  Q(x) = xáµ€ A x is **convex** â†’ guarantees a unique global minimum.  

---

### ğŸ”¹ Gradient & Optimization  
- Gradient of L(w):  
  âˆ‡L(w) = Xáµ€(Xw âˆ’ y)  
- Setting gradient = 0 â†’ Optimal weights w*:  
  w* = (Xáµ€X)â»Â¹ Xáµ€ y  

---

### ğŸ”¹ Applications in ML  
1. **Linear Regression:** Loss function is quadratic â†’ Convex optimization.  
2. **PCA:** Eigenvalue problems use quadratic forms for variance maximization.  
3. **SVMs:** Quadratic programming solves the margin maximization problem.  
4. **Neural Networks:** Loss surface analysis aids in optimization & training stability.  

---
