# ğŸ“˜ Day 8: Projections & Least Squares  

## ğŸ“Œ Agenda  
- Projection Matrices  
- Least Squares Solution (Normal Equation)  
- Linear Regression using NumPy & scikit-learn  
- Visualization & Applications in ML  

---

## ğŸ§¾ Key Concepts  

### ğŸ”¹ Projection Matrices  
- A projection matrix projects a vector onto the column space of a matrix \( X \).  
- Formula:  
  P = X(Xáµ€X)â»Â¹Xáµ€  
- Properties:  
  - PÂ² = P (idempotent)  
  - Páµ€ = P (symmetric)  

---

### ğŸ”¹ Least Squares Solution  
- Objective: Minimize the squared error ||y âˆ’ XÎ¸||Â².  
- Formula (Normal Equation):  
  Î¸ = (Xáµ€X)â»Â¹Xáµ€y  
- Geometric Interpretation: y is projected onto the column space of X.  

---

### ğŸ”¹ Linear Regression Relations  
- Predictions:  Å· = X_b Î¸  
- Residuals:  r = y âˆ’ Å·  

---

### ğŸ”¹ Visualization  
- **Data points (blue):** Original data  
- **Regression line (red):** Best-fit line using least squares  
- **Residuals (green):** Error between actual \( y \) and predicted \( \hat{y} \)  

---

### ğŸ”¹ Applications in Machine Learning  

1. **Linear Regression:**  
   - Projection matrices give the best linear fit minimizing error.  
   - Basis for many regression-based models.  

2. **Dimensionality Reduction:**  
   - Project data onto lower-dimensional subspaces while retaining information.  

3. **Noise Reduction:**  
   - Projection keeps the signal in the subspace, removing orthogonal noise.  

4. **Optimization & Estimation:**  
   - Used in least squares estimation, curve fitting, and statistical inference.  


