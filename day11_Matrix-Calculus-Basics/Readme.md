# Day 11: Matrix Calculus Basics  

## ğŸ“Œ Agenda  
- Introduction to Gradients & Jacobians  
- Manual computation for scalar and vector functions  
- Gradient Descent Visualization  
- Applications in Machine Learning  

---

## ğŸ§¾ Key Concepts  

### ğŸ”¹ Gradient  
- The gradient of a scalar function measures the rate of change of the function with respect to each variable.  
- Formula:  
  âˆ‡f(x) = [ âˆ‚f/âˆ‚xâ‚ , âˆ‚f/âˆ‚xâ‚‚ , ..., âˆ‚f/âˆ‚xâ‚™ ]áµ€  
- Example: For f(x, y) = xÂ² + yÂ²,  
  âˆ‡f(x, y) = [ 2x , 2y ]áµ€  

---

### ğŸ”¹ Jacobian  
- For a vector-valued function f: â„â¿ â†’ â„áµ, the Jacobian is the matrix of first-order partial derivatives.  
- Formula:  
  J = [ âˆ‚fáµ¢ / âˆ‚xâ±¼ ]  
- Example: For f(x, y) = [ xÂ² + y , x*y ],  
  J = [ [2x, 1], [y, x] ]  

---

### ğŸ”¹ Gradient Descent  
- Iterative optimization algorithm to minimize functions.  
- Formula:  
  xâ‚–â‚Šâ‚ = xâ‚– âˆ’ Î± âˆ‡f(xâ‚–)  
  where Î± = learning rate  

---

### ğŸ”¹ Application in ML: Linear Regression  
- Loss function (Mean Squared Error):  
  L(w) = (1/m) Î£ (y - Xw)Â²  
- Gradient w.r.t. weights:  
  âˆ‡w = âˆ’(2/m) Xáµ€ (y âˆ’ Xw)  
- Used in gradient-based optimization algorithms for training ML models.  

---

## ğŸ”¹ Usage in ML  

1. **Training Models:**  
   - Gradients are used in backpropagation for training neural networks.  

2. **Optimization:**  
   - Gradient descent and its variants (SGD, Adam) rely on these concepts.  

3. **Regression & Classification:**  
   - Computing loss gradients helps minimize errors in prediction models.  

4. **Deep Learning:**  
   - Automatic differentiation tools like TensorFlow/PyTorch compute gradients internally using these formulas.  

---
