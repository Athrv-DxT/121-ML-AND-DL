# Day 9: Positive Definite Matrices  

## ðŸ“Œ Agenda  
- Understanding Positive Definite (PD) Matrices  
- Covariance Matrices in Machine Learning  
- Cholesky Decomposition  
- Visualization with heatmap

---

## ðŸ§¾ Key Concepts  

### ðŸ”¹ Positive Definite Matrices  
- A symmetric matrix **A** is **positive definite** if:  
  xáµ€ A x > 0 for all x â‰  0  

- **Properties:**  
  1. All eigenvalues Î»áµ¢ > 0  
  2. Determinant det(A) > 0  
  3. Diagonal entries aáµ¢áµ¢ > 0  

---

### ðŸ”¹ Covariance Matrices  
- A covariance matrix Î£ is always **symmetric** and **positive semi-definite**.  
- Formula for covariance between two variables X and Y:  
  Cov(X,Y) = E[(X - Î¼â‚“)(Y - Î¼áµ§)]  

- In matrix form for n samples:  
  Î£ = (1 / n) * (X - Î¼)áµ€(X - Î¼)  

---

### ðŸ”¹ Cholesky Decomposition  
- Factorizes a positive definite matrix A into:  
  A = L Láµ€  
  where L is a **lower triangular** matrix.  

- Used in optimization, sampling, and solving linear systems efficiently.  

---

### ðŸ”¹ Applications in ML  

1. **Gaussian Processes:**  
   Covariance matrices in Gaussian Processes must be positive definite to ensure valid probability distributions.  

2. **Optimization Algorithms:**  
   Many optimization methods rely on PD Hessian matrices for convergence guarantees.  

3. **Numerical Stability:**  
   Cholesky decomposition helps solve linear systems stably and efficiently without directly computing inverses.  

---
