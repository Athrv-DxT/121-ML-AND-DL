# Day 32 â€“ Decision Trees ğŸŒ³

### ğŸ“ Concept
Decision Trees are supervised learning models used for **classification and regression**.  
They split the dataset into subsets based on feature values using measures of impurity.

---

### ğŸ“Œ Formulas

- **Entropy:**  **Entropy(S) = - Î£ páµ¢ logâ‚‚(páµ¢)**  
- **Information Gain:**  **IG(S, A) = Entropy(S) - Î£ (|Sáµ¥| / |S|) Ã— Entropy(Sáµ¥)**  
- **Gini Index:**  **Gini(S) = 1 - Î£ (páµ¢)Â²**

---

### âš¡ Key Notes
- Splitting stops when:
  - All samples belong to one class  
  - Or max depth/pruning criteria is reached  
- Trees are prone to **overfitting** â†’ solved by pruning or limiting depth.  
- **Feature importance** can be extracted after training.  
