# ðŸ“˜ Day 12: Recap of Linear Algebra from Day 01 to Day 11 + Mini Project

## ðŸ“Œ Agenda  
- Matrix Addition, Multiplication & Transpose  
- Rank & Null Space  
- Eigenvalues & Eigenvectors  
- Cholesky, LU, QR, and SVD Decomposition  
- Mean Squared Error (MSE) Loss  
- Gradients & Jacobians  
- Mini Project using above concepts

---

## ðŸ§¾ Key Concepts  

### ðŸ”¹ Matrix Operations  
- Matrix Addition:  C = A + B  
- Matrix Multiplication:  C = A Ã— B  
- Transpose:  Aáµ€  

---

### ðŸ”¹ Rank & Null Space  
- Rank of a matrix:  rank(A) = dimension of column space  

---

### ðŸ”¹ Eigenvalues & Eigenvectors  
- For A v = Î» v  
- Î» are eigenvalues, v are eigenvectors  

---

### ðŸ”¹ Decompositions  

1. **Cholesky Decomposition:**  
   A = L Láµ€  

2. **LU Decomposition:**  
   A = L U  

3. **QR Decomposition:**  
   A = Q R  

4. **Singular Value Decomposition (SVD):**  
   A = U Î£ Váµ€  

---

### ðŸ”¹ Mean Squared Error (MSE) Loss  
- Formula:  MSE = (1/n) Î£ (yáµ¢ âˆ’ Å·áµ¢)Â²  

---

### ðŸ”¹ Gradients & Jacobians  
- Gradient of f(x, y):  âˆ‡f = [âˆ‚f/âˆ‚x, âˆ‚f/âˆ‚y]  
- Jacobian for vector-valued function F(x): J = [âˆ‚Fáµ¢/âˆ‚xâ±¼]  

---

### ðŸ”¹ Applications in Machine Learning  
1. **Optimization:** Gradient Descent uses gradients to minimize loss.  
2. **PCA & Dimensionality Reduction:** Based on eigenvalues & SVD.  
3. **Regression Models:** MSE as loss function.  
4. **Numerical Computations:** LU, QR, and SVD for solving systems efficiently.  

---
