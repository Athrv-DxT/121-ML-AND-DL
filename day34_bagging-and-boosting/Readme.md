# Day 34 ‚Äì Bagging & Boosting

## üìå Concepts Covered:
- **Bagging (Bootstrap Aggregating)**  
- **Boosting: AdaBoost & Gradient Boosting**  

---

## üßæ Bagging (Bootstrap Aggregating)

- Bagging builds multiple models independently and averages their predictions.  
- Reduces variance and helps avoid overfitting.  

**Formula (Bagging prediction):**  
**≈∑ = (1 / B) Œ£ f·µ¢(x)**  
where `B` = number of bootstrap models, and `f·µ¢(x)` = prediction of model *i*.  

---

## üßæ AdaBoost (Adaptive Boosting)

- Sequentially builds weak learners (usually decision stumps).  
- Each classifier is assigned a weight depending on accuracy.  
- Misclassified points get **higher weights** in the next round.  

**Weight update rule:**  
**w·µ¢ ‚Üê w·µ¢ * exp(Œ±‚Çú * I(y·µ¢ ‚â† h‚Çú(x·µ¢)))**  

where:  
- `w·µ¢` = weight of sample *i*  
- `Œ±‚Çú = (1/2) * ln((1 - Œµ‚Çú) / Œµ‚Çú)` (classifier weight)  
- `Œµ‚Çú` = error rate of classifier *t*  

---

## üßæ Gradient Boosting

- Models are built sequentially to correct errors of previous models.  
- Each new model fits the **residuals (errors)** of the last.  

**Update rule:**  
**F‚Çò(x) = F‚Çò‚Çã‚ÇÅ(x) + Œ∑ * h‚Çò(x)**  

where:  
- `Œ∑` = learning rate  
- `h‚Çò(x)` = weak learner at step *m*  
- `F‚Çò(x)` = boosted model after step *m*  

---

## üîë Key Differences

- **Bagging:** Parallel training, reduces variance.  
- **Boosting:** Sequential training, reduces bias & variance.  

---
